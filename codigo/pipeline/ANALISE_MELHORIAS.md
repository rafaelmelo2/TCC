# An√°lise de Melhorias Necess√°rias

**Data:** 2026-01-27  
**Status:** An√°lise dos resultados e identifica√ß√£o de melhorias

---

## üî¥ Problemas Identificados

### 1. F1=0.0 e MCC=0.0 em alguns folds
**Problema:** PETR4 Folds 2 e 3 t√™m F1=0.0 e MCC=0.0
- **Causa:** Modelo prevendo sempre a mesma classe (provavelmente sempre "baixa")
- **Impacto:** Modelo n√£o est√° aprendendo padr√µes reais, apenas explorando desbalanceamento

**Solu√ß√£o:**
- Melhorar class weights (usar sklearn.utils.class_weight)
- Adicionar monitoramento de distribui√ß√£o de previs√µes
- Considerar focal loss ao inv√©s de binary crossentropy

### 2. Acur√°cias baixas em alguns folds
- PETR4 Fold 3: 47.15% (abaixo do baseline)
- ITUB4 Fold 5: 50.00% (exatamente no acaso)

**Poss√≠veis causas:**
- Per√≠odos dif√≠ceis do mercado
- Modelo n√£o convergindo adequadamente
- Features n√£o informativas para esses per√≠odos

---

## ‚úÖ O que J√Å est√° implementado (do TCC)

1. ‚úÖ Walk-forward validation (Se√ß√£o 4.4)
2. ‚úÖ Otimiza√ß√£o bayesiana (Optuna) (Se√ß√£o 4.4.2)
3. ‚úÖ AdamW optimizer (Se√ß√£o 4.4)
4. ‚úÖ Gradient clipping (Se√ß√£o 4.4)
5. ‚úÖ Early stopping (Se√ß√£o 4.4)
6. ‚úÖ ReduceLROnPlateau (Se√ß√£o 4.4)
7. ‚úÖ Class weights b√°sicos (Se√ß√£o 4.4)
8. ‚úÖ Banda morta (0.1%) (Se√ß√£o 4.2)
9. ‚úÖ Salvamento de modelos por fold

---

## ‚è≥ O que FALTA implementar (do TCC)

### 1. Cosine Annealing Scheduler (Se√ß√£o 4.4) üî¥ CR√çTICO
**Status:** N√£o implementado  
**Benef√≠cio esperado:** +1-3% acur√°cia  
**Prioridade:** ALTA

**Implementa√ß√£o necess√°ria:**
```python
from tensorflow.keras.callbacks import LearningRateScheduler
from tensorflow.keras.optimizers.schedules import CosineDecayRestarts

# Cosine annealing com restarts
cosine_schedule = CosineDecayRestarts(
    initial_learning_rate=learning_rate,
    first_decay_steps=epochs // 2,
    t_mul=2.0,
    m_mul=1.0,
    alpha=1e-7
)
```

### 2. Melhorias em Class Weights üî¥ CR√çTICO
**Status:** Implementa√ß√£o b√°sica (pode melhorar)  
**Problema:** Alguns folds ainda colapsam para mesma classe  
**Prioridade:** ALTA

**Melhorias:**
- Usar `sklearn.utils.class_weight.compute_class_weight`
- Adicionar monitoramento de distribui√ß√£o de previs√µes
- Considerar focal loss para classes desbalanceadas

### 3. Features Adicionais (Se√ß√£o 4.2) üü° M√âDIO
**Status:** N√£o implementado  
**Benef√≠cio esperado:** +2-5% acur√°cia  
**Prioridade:** M√âDIA

**Features a adicionar:**
- Amplitude high-low normalizada
- Varia√ß√µes de volume (volume_t / volume_ma)
- Hora do dia (sin/cos encoding)
- Indicador de fase do preg√£o (abertura/meio/fechamento)

### 4. Ensemble de Modelos (Se√ß√£o 3.2) üü° M√âDIO
**Status:** Parcialmente implementado (script existe mas n√£o integrado)  
**Benef√≠cio esperado:** +3-5% acur√°cia  
**Prioridade:** M√âDIA

**Implementa√ß√£o:**
- Voting dos 5 folds treinados
- M√©dia ponderada de probabilidades
- Metaclassificador (opcional)

### 5. Retreinamento no Maior Prefixo (Se√ß√£o 4.4) üü¢ BAIXO
**Status:** N√£o implementado  
**Benef√≠cio:** Modelo de produ√ß√£o mais robusto  
**Prioridade:** BAIXA

**Estrat√©gia:**
- Ap√≥s walk-forward, retreinar com TODOS os dados
- Usar melhores hiperpar√¢metros (m√©dia dos 5 folds)

---

## üìã Plano de A√ß√£o Imediato

### Fase 1: Corre√ß√µes Cr√≠ticas (HOJE)
1. ‚úÖ Implementar Cosine Annealing Scheduler
2. ‚úÖ Melhorar class weights (sklearn)
3. ‚úÖ Adicionar monitoramento de distribui√ß√£o de previs√µes
4. ‚úÖ Testar em um fold problem√°tico

### Fase 2: Melhorias Adicionais (PR√ìXIMOS DIAS)
1. Adicionar features extras
2. Implementar ensemble
3. Retreinar modelos com melhorias

---

## üéØ Resultados Esperados ap√≥s Melhorias

**Atual:**
- VALE3: 53.31%
- PETR4: 50.57%
- ITUB4: 52.27%

**Ap√≥s Fase 1 (Cosine + Class Weights):**
- Esperado: +2-4% ‚Üí 55-57% m√©dia

**Ap√≥s Fase 2 (Features + Ensemble):**
- Esperado: +5-8% ‚Üí 58-62% m√©dia

---

## üìù Notas T√©cnicas

### Por que F1=0.0 acontece?
Quando o modelo prev√™ sempre a mesma classe (ex: sempre "baixa"), temos:
- Precision = 0 (nenhum verdadeiro positivo)
- Recall = 0 (nenhum verdadeiro positivo)
- F1 = 2 * (0 * 0) / (0 + 0) = 0/0 = 0 (por defini√ß√£o)

### Como evitar?
1. Class weights mais agressivos
2. Focal loss (penaliza mais erros em classes minorit√°rias)
3. Oversampling/undersampling
4. Monitoramento durante treinamento
