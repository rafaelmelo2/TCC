# Guia de Melhorias - Como Aumentar a Acur√°cia

**Data:** 2026-01-23  
**Status:** Refer√™ncia (como aumentar acur√°cia, t√©cnicas TCC)

---

## 1. Sobre Aumentar Epochs

### Resposta Direta

**Com early stopping, aumentar epochs N√ÉO prejudica:**

- ‚úÖ Para automaticamente quando n√£o h√° melhoria (patience=10)
- ‚úÖ Apenas define limite m√°ximo de treinamento
- ‚úÖ √ötil para learning rates baixos que precisam de mais tempo
- ‚è±Ô∏è Aumenta tempo m√°ximo, mas para antes se convergir

**Configura√ß√£o atual:**
```python
epochs = 100  # M√°ximo
patience = 10  # Para se 10 √©pocas sem melhoria
```

**Resultado t√≠pico:** Para entre 20-50 √©pocas

**Recomenda√ß√£o:** Aumentar para 150-200 epochs no pr√≥ximo treino

---

## 2. Como Melhorar Acur√°cia (Seguindo TCC)

### TCC Se√ß√£o 4.4 - T√©cnicas Recomendadas

‚úÖ **J√Å IMPLEMENTADO:**
1. Gradient clipping (clipnorm=1.0)
2. AdamW optimizer
3. Early stopping
4. Reduce LR on plateau
5. Class weights (balanceamento)
6. Dropout regularization
7. **Salvamento autom√°tico de modelos** ‚Üê NOVO!

üîÑ **PR√ìXIMO A IMPLEMENTAR:**
1. Cosine annealing scheduler (TCC Se√ß√£o 4.4)
2. Features adicionais (amplitude, volume)
3. Ensemble de modelos (TCC Se√ß√£o 3.2)
4. Retreinamento no maior prefixo

---

## 3. IMPORTANTE: Modelos Salvos!

### Problema Resolvido

Antes: Treinamento de 2 horas sem salvar modelos  
Agora: **Salvamento autom√°tico a cada fold!**

### Onde est√£o os modelos?

```
models/
‚îî‚îÄ‚îÄ VALE3/
    ‚îî‚îÄ‚îÄ cnn_lstm/
        ‚îú‚îÄ‚îÄ fold_1_checkpoint.keras  ‚Üê Melhor modelo do fold 1
        ‚îú‚îÄ‚îÄ fold_2_checkpoint.keras
        ‚îú‚îÄ‚îÄ fold_3_checkpoint.keras
        ‚îú‚îÄ‚îÄ fold_4_checkpoint.keras
        ‚îî‚îÄ‚îÄ fold_5_checkpoint.keras
```

### Como usar os modelos salvos?

```python
from tensorflow import keras

# Carregar modelo do fold 5 (melhor: 56.82%)
model = keras.models.load_model('models/VALE3/cnn_lstm/fold_5_checkpoint.keras')

# Fazer previs√µes
predictions = model.predict(X_new)
directions = np.where(predictions > 0.5, 1, -1)
```

### Analisar modelos salvos

```bash
# Script criado para an√°lise
uv run python src/scripts/analisar_modelos_salvos.py --ativo VALE3 --modelo cnn_lstm
```

---

## 4. Plano de Melhorias Sequencial

### Fase 1: Melhorias Imediatas (AGORA)

**O que fazer:**
1. ‚úÖ Salvamento implementado
2. ‚úÖ Gradient clipping implementado
3. Retreinar com configura√ß√µes melhoradas

**Comando:**
```bash
# Treinar com mais trials e epochs
uv run python src/train.py \
    --ativo VALE3 \
    --modelo cnn_lstm \
    --optuna \
    --n-trials 50 \
    --epochs 150
```

**Tempo:** ~3-4 horas  
**Melhoria esperada:** 52.51% ‚Üí 54-56%  
**Modelos salvos:** Sim, automaticamente!

### Fase 2: Features Adicionais (Depois)

**Implementar:**
- Amplitude high-low normalizada
- Varia√ß√µes de volume
- Hora do dia (sin/cos)
- Fase do preg√£o

**Tempo:** 1h implementa√ß√£o + 2h treino  
**Melhoria esperada:** +2-3% acur√°cia

### Fase 3: Ensemble (Depois)

**Estrat√©gia:**
- Usar os 5 modelos salvos (um por fold)
- Voting ou m√©dia de probabilidades
- Pode chegar a 58-60% de acur√°cia

**Tempo:** ~30 min implementa√ß√£o  
**Melhoria esperada:** +3-5% acur√°cia

---

## 5. Melhorias Priorit√°rias

### Op√ß√£o A: Retreinar com Melhorias Implementadas ‚≠ê RECOMENDADO

```bash
# Melhor custo-benef√≠cio
uv run python src/train.py \
    --ativo VALE3 \
    --modelo cnn_lstm \
    --optuna \
    --n-trials 50 \
    --epochs 150
```

**Por que:**
- Aproveita gradient clipping (novo)
- Mais trials = melhor hiperpar√¢metros
- Mais epochs = melhor converg√™ncia
- Modelos salvos automaticamente
- **Melhoria esperada: 54-56% acur√°cia**

### Op√ß√£o B: Ensemble com Modelos Atuais

```python
# Usar os 5 modelos que voc√™ j√° tem
# Fazer m√©dia das previs√µes
# Melhoria esperada: 54-55% acur√°cia
```

**Por que:**
- R√°pido (30 min)
- N√£o precisa retreinar
- Aproveita trabalho j√° feito

### Op√ß√£o C: Implementar Schedulers + Retreinar

```python
# Adicionar cosine scheduler
# Retreinar tudo
# Melhoria esperada: 55-57% acur√°cia
```

**Por que:**
- M√°ximo de melhoria t√©cnica
- Segue TCC rigorosamente
- Mais demorado (~1h implementa√ß√£o + 4h treino)

---

## 6. Expectativas Realistas

### Literatura de Previs√£o Intradi√°ria

| M√©todo | Acur√°cia T√≠pica |
|--------|-----------------|
| Baseline (naive) | ~50% |
| ARIMA | 48-52% |
| LSTM single | 52-56% |
| CNN-LSTM | 54-58% |
| Ensemble | 56-62% |
| Estado da arte | 58-65% |

**Nosso resultado atual:**
- 52.51% com CNN-LSTM ‚Üí **dentro do esperado**
- Com melhorias: 54-58% ‚Üí **realista**
- Com ensemble: 56-60% ‚Üí **otimista**

### Por que n√£o 90%+?

1. **Mercado eficiente**: Se fosse f√°cil, todos fariam
2. **Ru√≠do intradi√°rio**: Movimentos de 15min s√£o muito vol√°teis
3. **Limita√ß√£o fundamental**: Pre√ßo futuro depende de fatores desconhecidos

**Mas 55% j√° √© rent√°vel!**
- Com boa gest√£o de risco
- Usando custos de transa√ß√£o
- Stop loss adequado

---

## 7. Comandos R√°pidos

### Retreinar AGORA com Melhorias

```bash
cd ~/Arquivos/TCC/codigo/pipeline

# CNN-LSTM melhorado (RECOMENDADO)
uv run python src/train.py --ativo VALE3 --modelo cnn_lstm --optuna --n-trials 50 --epochs 150

# Verificar modelos salvos depois
ls -lh models/VALE3/cnn_lstm/

# Analisar modelos
uv run python src/scripts/analisar_modelos_salvos.py --ativo VALE3 --modelo cnn_lstm
```

### Treinar LSTM Puro (Baseline 3)

```bash
# Para compara√ß√£o
uv run python src/train.py --ativo VALE3 --modelo lstm --optuna --n-trials 30 --epochs 150
```

### Treinar Outros Ativos

```bash
# PETR4
uv run python src/train.py --ativo PETR4 --modelo cnn_lstm --optuna --n-trials 30 --epochs 150

# ITUB4
uv run python src/train.py --ativo ITUB4 --modelo cnn_lstm --optuna --n-trials 30 --epochs 150
```

---

## 8. O Que Mudou

### Antes (Treinamento Anterior)
```
‚ùå Sem salvamento de modelos
‚ùå Sem gradient clipping
‚ùå Patience baixo (5)
‚ùå Poucas epochs (30)
‚Üí Acur√°cia: ~52% mas modelos perdidos
```

### Agora (Com Melhorias)
```
‚úÖ Salvamento autom√°tico por fold
‚úÖ Gradient clipping (clipnorm=1.0)
‚úÖ Patience adequado (10)
‚úÖ Mais epochs (100-150)
‚úÖ AdamW optimizer
‚Üí Acur√°cia esperada: 54-56%
‚Üí Modelos salvos e utiliz√°veis!
```

---

## 9. Resumo das Melhorias

### T√©cnicas do TCC Implementadas

| T√©cnica | Status | Impacto Esperado |
|---------|--------|------------------|
| Walk-forward | ‚úÖ Implementado | Essencial |
| Optuna bayesiano | ‚úÖ Implementado | +2-3% |
| AdamW | ‚úÖ Implementado | +1% |
| Gradient clipping | ‚úÖ Implementado | +0.5-1% |
| Early stopping | ‚úÖ Implementado | Previne overfit |
| Dropout | ‚úÖ Implementado | Regulariza√ß√£o |
| Class weights | ‚úÖ Implementado | Balanceamento |
| Salvamento | ‚úÖ Implementado | Preserva trabalho |
| Cosine scheduler | ‚è≥ Pr√≥ximo | +1-2% |
| Ensemble | ‚è≥ Pr√≥ximo | +3-5% |
| Features extras | ‚è≥ Pr√≥ximo | +2-3% |

**Total esperado:** 52.51% ‚Üí 58-62% (com todas as t√©cnicas)

---

## 10. Minha Recomenda√ß√£o

### Op√ß√£o 1: Retreinar AGORA ‚≠ê MELHOR

```bash
uv run python src/train.py --ativo VALE3 --modelo cnn_lstm --optuna --n-trials 50 --epochs 150
```

**Por que:**
- Aproveita gradient clipping (NOVO)
- Modelos ser√£o salvos (NOVO)
- Mais trials = melhores hiperpar√¢metros
- Tempo: 3-4 horas
- **Melhoria esperada: 54-56%**

### Op√ß√£o 2: Implementar Schedulers DEPOIS Retreinar

**Etapas:**
1. Implementar cosine scheduler (~30 min)
2. Retreinar (~4 horas)
3. **Melhoria esperada: 55-58%**

### Op√ß√£o 3: Ensemble com Modelos Atuais

**Se quiser resultado r√°pido:**
- Implementar voting dos 5 folds (~30 min)
- N√£o precisa retreinar
- **Melhoria esperada: 54-55%**

---

## 11. Documenta√ß√£o Completa

- [Melhorias T√©cnicas](../implementacoes/melhorias_tecnicas_2026_01_23.md)
- [Corre√ß√µes do Treinamento](../implementacoes/correcoes_treinamento_2026_01_23.md)

---

**O que voc√™ quer fazer agora?**

1. Retreinar com as melhorias (RECOMENDO)
2. Implementar ensemble r√°pido
3. Implementar schedulers antes de retreinar
