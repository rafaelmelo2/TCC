# An√°lise de Melhorias Necess√°rias

**Data:** 2026-01-27  
**Status:** Refer√™ncia (diagn√≥stico F1/MCC e melhorias)

---

## 1. Problemas Identificados

### 1. F1=0.0 e MCC=0.0 em alguns folds
**Problema:** PETR4 Folds 2 e 3 t√™m F1=0.0 e MCC=0.0
- **Causa:** Modelo prevendo sempre a mesma classe (provavelmente sempre "baixa")
- **Impacto:** Modelo n√£o est√° aprendendo padr√µes reais, apenas explorando desbalanceamento

**Solu√ß√£o:**
- Melhorar class weights (usar sklearn.utils.class_weight)
- Adicionar monitoramento de distribui√ß√£o de previs√µes
- Considerar focal loss ao inv√©s de binary crossentropy

### 2. Acur√°cias baixas em alguns folds
- PETR4 Fold 3: 47.15% (abaixo do baseline)
- ITUB4 Fold 5: 50.00% (exatamente no acaso)

**Poss√≠veis causas:**
- Per√≠odos dif√≠ceis do mercado
- Modelo n√£o convergindo adequadamente
- Features n√£o informativas para esses per√≠odos

---

## 2. O que J√Å est√° implementado (do TCC)

1. ‚úÖ Walk-forward validation (Se√ß√£o 4.4)
2. ‚úÖ Otimiza√ß√£o bayesiana (Optuna) (Se√ß√£o 4.4.2)
3. ‚úÖ AdamW optimizer (Se√ß√£o 4.4)
4. ‚úÖ Gradient clipping (Se√ß√£o 4.4)
5. ‚úÖ Early stopping (Se√ß√£o 4.4)
6. ‚úÖ ReduceLROnPlateau (Se√ß√£o 4.4)
7. ‚úÖ Class weights b√°sicos (Se√ß√£o 4.4)
8. ‚úÖ Banda morta (0.1%) (Se√ß√£o 4.2)
9. ‚úÖ Salvamento de modelos por fold

---

## 3. O que FALTA implementar (do TCC)

**Nota:** Itens 1 e 2 abaixo **j√° foram implementados** em 2026-01-27. Ver [melhorias_criticas_2026_01_27.md](../implementacoes/melhorias_criticas_2026_01_27.md) e [RESUMO_MELHORIAS.md](RESUMO_MELHORIAS.md) (ambos neste hist√≥rico).

### 1. Cosine Annealing Scheduler (Se√ß√£o 4.4) ‚úÖ IMPLEMENTADO
**Status:** Implementado (2026-01-27) em `src/train.py` e `src/utils/optuna_optimizer.py`  
**Implementa√ß√£o:** `CosineDecayRestarts` do TensorFlow + `LearningRateScheduler` callback.

### 2. Melhorias em Class Weights ‚úÖ IMPLEMENTADO
**Status:** Implementado (2026-01-27): `sklearn.utils.class_weight.compute_class_weight`, monitoramento de distribui√ß√£o de previs√µes, focal loss (`src/utils/focal_loss.py`). Alguns folds ainda podem colapsar (limita√ß√£o do per√≠odo/dados).

### 3. Features Adicionais (Se√ß√£o 4.2) üü° M√âDIO
**Status:** N√£o implementado  
**Benef√≠cio esperado:** +2-5% acur√°cia  
**Prioridade:** M√âDIA

**Features a adicionar:**
- Amplitude high-low normalizada
- Varia√ß√µes de volume (volume_t / volume_ma)
- Hora do dia (sin/cos encoding)
- Indicador de fase do preg√£o (abertura/meio/fechamento)

### 4. Ensemble de Modelos (Se√ß√£o 3.2) üü° M√âDIO
**Status:** Parcialmente implementado (script existe mas n√£o integrado)  
**Benef√≠cio esperado:** +3-5% acur√°cia  
**Prioridade:** M√âDIA

**Implementa√ß√£o:**
- Voting dos 5 folds treinados
- M√©dia ponderada de probabilidades
- Metaclassificador (opcional)

### 5. Retreinamento no Maior Prefixo (Se√ß√£o 4.4) üü¢ BAIXO
**Status:** N√£o implementado  
**Benef√≠cio:** Modelo de produ√ß√£o mais robusto  
**Prioridade:** BAIXA

**Estrat√©gia:**
- Ap√≥s walk-forward, retreinar com TODOS os dados
- Usar melhores hiperpar√¢metros (m√©dia dos 5 folds)

---

## 4. Plano de A√ß√£o Imediato

### Fase 1: Corre√ß√µes Cr√≠ticas (HOJE)
1. ‚úÖ Implementar Cosine Annealing Scheduler
2. ‚úÖ Melhorar class weights (sklearn)
3. ‚úÖ Adicionar monitoramento de distribui√ß√£o de previs√µes
4. ‚úÖ Testar em um fold problem√°tico

### Fase 2: Melhorias Adicionais (PR√ìXIMOS DIAS)
1. Adicionar features extras
2. Implementar ensemble
3. Retreinar modelos com melhorias

---

## 5. Resultados Esperados ap√≥s Melhorias

**Atual:**
- VALE3: 53.31%
- PETR4: 50.57%
- ITUB4: 52.27%

**Ap√≥s Fase 1 (Cosine + Class Weights):**
- Esperado: +2-4% ‚Üí 55-57% m√©dia

**Ap√≥s Fase 2 (Features + Ensemble):**
- Esperado: +5-8% ‚Üí 58-62% m√©dia

---

## 6. Notas T√©cnicas

### Por que F1=0.0 acontece?
Quando o modelo prev√™ sempre a mesma classe (ex: sempre "baixa"), temos:
- Precision = 0 (nenhum verdadeiro positivo)
- Recall = 0 (nenhum verdadeiro positivo)
- F1 = 2 * (0 * 0) / (0 + 0) = 0/0 = 0 (por defini√ß√£o)

### Como evitar?
1. Class weights mais agressivos
2. Focal loss (penaliza mais erros em classes minorit√°rias)
3. Oversampling/undersampling
4. Monitoramento durante treinamento
